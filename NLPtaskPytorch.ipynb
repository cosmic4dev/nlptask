{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1LlT8a4wKDQy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "자연어 처리 데이터 증강 라이브러리\n",
        "NLPAUG\n",
        "\n"
      ],
      "metadata": {
        "id": "UGFtqqFIoGrz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy requests nlpaug transformers sacremoses nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roYvxm-eoO2i",
        "outputId": "242c74f7-0e9d-48b9-9b94-4ebf00a50caf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: nlpaug in /usr/local/lib/python3.10/dist-packages (1.1.11)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.10/dist-packages (0.1.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.6.2)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (2.0.3)\n",
            "Requirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (5.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.4.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.12.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->nlpaug) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests) (1.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 삽입: 문장 의미에 영향을 끼치지 않는 수식어 등을 추가하는 방법\n",
        "import nlpaug.augmenter.word as naw\n",
        "\n",
        "texts= [\n",
        "    \"Those who can imagine anything, can create the impossible.\",\n",
        "    \"We can only see a short distance ahead, but we can see plenty there that needs to be done.\",\n",
        "    \"If a machine is expected to be infallible, it cannot also be intelligent.\",\n",
        "]\n",
        "\n",
        "aug = naw.ContextualWordEmbsAug(model_path='bert-base-uncased', action=\"insert\")\n",
        "augmented_text = aug.augment(texts)\n",
        "\n",
        "for text, augmented in zip(texts, augmented_text):\n",
        "    print(f\"src : {text}\")\n",
        "    print(f\"dst : {augmented}\")\n",
        "    print(\"----------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umz1dYcgoaxl",
        "outputId": "d26817c7-3b3b-4e1b-9697-68e59d9f2e6f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "src : Those who can imagine anything, can create the impossible.\n",
            "dst : those beings who can imagine practically anything, can again create only the impossible.\n",
            "----------------\n",
            "src : We can only see a short distance ahead, but we can see plenty there that needs to be done.\n",
            "dst : we can now only see a short distance ahead, general but we all can easily see plenty down there that no needs to help be done.\n",
            "----------------\n",
            "src : If a machine is expected to be infallible, it cannot also be intelligent.\n",
            "dst : even if yet a massive machine system is expected only to be infallible, surely it cannot also be intelligent.\n",
            "----------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "위 ContextualWordEmbsAug 클래스는 BERT 모델을 활용해 단어를 삽입하는 기능을 제공한다.  \n",
        "model-path는 bert-base-uncased나 distilbert-base-uncased를 인수로 활용해 적용하며 허깅 페이스나 모델을 자동으로 다운로드해 불러온다.\n"
      ],
      "metadata": {
        "id": "LNHh-KZPoq_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 문자 삭제\n",
        "import nlpaug.augmenter.char as nac\n",
        "\n",
        "texts= [\n",
        "    \"Those who can imagine anything, can create the impossible.\",\n",
        "    \"We can only see a short distance ahead, but we can see plenty there that needs to be done.\",\n",
        "    \"If a machine is expected to be infallible, it cannot also be intelligent.\",\n",
        "]\n",
        "\n",
        "aug = nac.RandomCharAug(action=\"delete\")\n",
        "augmented_text = aug.augment(texts)\n",
        "\n",
        "for text, augmented in zip(texts, augmented_text):\n",
        "    print(f\"src : {text}\")\n",
        "    print(f\"dst : {augmented}\")\n",
        "    print(\"----------------\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5WTFb8QrVlL",
        "outputId": "78de8849-5e60-4c54-c6ae-d46f8f5a5838"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "src : Those who can imagine anything, can create the impossible.\n",
            "dst : Tse who can iage aytng, can create the issible.\n",
            "----------------\n",
            "src : We can only see a short distance ahead, but we can see plenty there that needs to be done.\n",
            "dst : We can oy see a sot distance ahd, but we can see lent tee that nes to be de.\n",
            "----------------\n",
            "src : If a machine is expected to be infallible, it cannot also be intelligent.\n",
            "dst : If a machine is eeced to be iflible, it cano ao be ntllign.\n",
            "----------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "추가로 교체(substite), 교체(swap)기능도 제공한다.  \n",
        "교체는 단어나 문자의 위치를 교환하는 방법이다. 가령 \"문제점을 찾지 말고 해결책을 찾으라\"라는 문장에서 \"해결책을 찾으라 문제점을 찾지 말고\"로 변경될 수 있다."
      ],
      "metadata": {
        "id": "7RihWvIxsB27"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 이미지 데이터\n",
        "imgaug라이브러리를 사용한다."
      ],
      "metadata": {
        "id": "XwyaUMSDtC0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imgaug\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6MtRnCs00Oi",
        "outputId": "d38065e4-62ed-4b95-b426-531a6b831259"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imgaug in /usr/local/lib/python3.10/dist-packages (0.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from imgaug) (1.16.0)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.10/dist-packages (from imgaug) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from imgaug) (1.11.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from imgaug) (9.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from imgaug) (3.7.1)\n",
            "Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.10/dist-packages (from imgaug) (0.19.3)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from imgaug) (4.8.0.76)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (from imgaug) (2.31.6)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.10/dist-packages (from imgaug) (2.0.4)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.14.2->imgaug) (3.3)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.14.2->imgaug) (2024.6.18)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.14.2->imgaug) (1.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.14.2->imgaug) (24.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug) (2.8.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "    transforms.Resize((512, 512)),\n",
        "    transforms.ToTensor()\n",
        "    ]\n",
        ")\n",
        "\n",
        "#image = Image.open(\"image.jpg\")\n",
        "#image = transform(image)"
      ],
      "metadata": {
        "id": "KqpZ5rs31TK0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Movie review Data set preprocessing\n"
      ],
      "metadata": {
        "id": "sAAqxUCxJvwL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 문장 분류 모델 (using lstm)\n"
      ],
      "metadata": {
        "id": "9Y7q53klmhtn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "class SentenceClassifier(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_vocab,\n",
        "        hidden_dim,\n",
        "        embedding_dim,\n",
        "        n_layers,\n",
        "        dropout=0.5,\n",
        "        bidirectional=True,\n",
        "        model_type=\"lstm\"\n",
        "      ):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=n_vocab,\n",
        "            embedding_dim=embedding_dim,\n",
        "            padding_idx=0\n",
        "        )\n",
        "\n",
        "\n",
        "        if model_type == 'rnn':\n",
        "            self.model = nn.RNN(\n",
        "                input_size=embedding_dim,\n",
        "                hidden_size=hidden_dim,\n",
        "                num_layers=n_layers,\n",
        "                dropout=dropout,\n",
        "                bidirectional=bidirectional,\n",
        "                batch_first=True,\n",
        "            )\n",
        "        elif model_type == \"lstm\":\n",
        "            self.model = nn.LSTM(\n",
        "                input_size=embedding_dim,\n",
        "                hidden_size=hidden_dim,\n",
        "                num_layers=n_layers,\n",
        "                dropout=dropout,\n",
        "                bidirectional=bidirectional,\n",
        "                batch_first=True,\n",
        "        )\n",
        "\n",
        "        if bidirectional:\n",
        "            self.classifier = nn.Linear(hidden_dim * 2, 1)\n",
        "        else:\n",
        "            self.classifier = nn.Linear(hidden_dim, 1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeddings = self.embedding(inputs)\n",
        "        output, _ = self.model(embeddings)\n",
        "        last_output = output[:, -1, :]\n",
        "        last_output = self.dropout(last_output)\n",
        "        logits = self.classifier(last_output)\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "ixWesoS0_hn5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "!pip install Korpora\n",
        "import pandas as pd\n",
        "from Korpora import Korpora\n",
        "\n",
        "corpus = Korpora.load(\"nsmc\")\n",
        "corpus_df = pd.DataFrame(corpus.test)\n",
        "corpus_df.head()\n",
        "\n",
        "\n",
        "train = corpus_df.sample(frac=0.9, random_state=42)\n",
        "test = corpus_df.drop(train.index)\n",
        "\n",
        "print(train.head(5).to_markdown())\n",
        "print(\"Training Data size :\", len(train))\n",
        "print(\"Testing Data size :\", len(test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0-a3SJ1Bv0F",
        "outputId": "363e8b3e-ca2a-4cdc-e4ca-da103642e79c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Korpora in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Requirement already satisfied: dataclasses>=0.6 in /usr/local/lib/python3.10/dist-packages (from Korpora) (0.6)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from Korpora) (1.25.2)\n",
            "Requirement already satisfied: tqdm>=4.46.0 in /usr/local/lib/python3.10/dist-packages (from Korpora) (4.66.4)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from Korpora) (2.31.0)\n",
            "Requirement already satisfied: xlrd>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from Korpora) (2.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->Korpora) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->Korpora) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->Korpora) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->Korpora) (2024.6.2)\n",
            "\n",
            "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
            "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
            "\n",
            "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
            "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
            "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
            "\n",
            "    # Description\n",
            "    Author : e9t@github\n",
            "    Repository : https://github.com/e9t/nsmc\n",
            "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
            "\n",
            "    Naver sentiment movie corpus v1.0\n",
            "    This is a movie review dataset in the Korean language.\n",
            "    Reviews were scraped from Naver Movies.\n",
            "\n",
            "    The dataset construction is based on the method noted in\n",
            "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
            "\n",
            "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
            "\n",
            "    # License\n",
            "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
            "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
            "\n",
            "[Korpora] Corpus `nsmc` is already installed at /root/Korpora/nsmc/ratings_train.txt\n",
            "[Korpora] Corpus `nsmc` is already installed at /root/Korpora/nsmc/ratings_test.txt\n",
            "|       | text                                                                                     |   label |\n",
            "|------:|:-----------------------------------------------------------------------------------------|--------:|\n",
            "| 33553 | 모든 편견을 날려 버리는 가슴 따뜻한 영화. 로버트 드 니로, 필립 세이모어 호프만 영원하라. |       1 |\n",
            "|  9427 | 무한 리메이크의 소재. 감독의 역량은 항상 그 자리에...                                    |       0 |\n",
            "|   199 | 신날 것 없는 애니.                                                                       |       0 |\n",
            "| 12447 | 잔잔 격동                                                                                |       1 |\n",
            "| 39489 | 오랜만에 찾은 주말의 명화의 보석                                                         |       1 |\n",
            "Training Data size : 45000\n",
            "Testing Data size : 5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data Tokenizing & building a word dictionary\n",
        "!pip install konlpy\n",
        "from konlpy.tag import Okt\n",
        "from collections import Counter\n",
        "\n",
        "def build_vocab(corpus, n_vocab, special_tokens):\n",
        "  counter = Counter()\n",
        "  for tokens in corpus:\n",
        "    counter.update(tokens)\n",
        "\n",
        "  vocab = special_tokens\n",
        "  for token, count in counter.most_common(n_vocab):\n",
        "    vocab.append(token)\n",
        "\n",
        "  return vocab\n",
        "\n",
        "tokenizer = Okt()\n",
        "train_tokens = [tokenizer.morphs(review) for review in train.text]\n",
        "test_tokens = [tokenizer.morphs(review) for review in test.text]\n",
        "\n",
        "vocab = build_vocab(train_tokens, 10000, [\"<pad>\", \"<unk>\"])\n",
        "token_to_id = {token: idx for idx, token in enumerate(vocab)}\n",
        "id_to_token = {idx: token for idx, token in enumerate(vocab)}\n",
        "\n",
        "print(vocab[:10])\n",
        "print(len(vocab))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZiyFesXBQ7M",
        "outputId": "95a6cb74-48cb-4f65-badd-a6e0a795a635"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.5.0)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.4)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (24.1)\n",
            "['<pad>', '<unk>', '.', '이', '영화', '의', '..', '가', '에', '...']\n",
            "10002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# integer encoding & padding\n",
        "import numpy as np\n",
        "\n",
        "def pad_sequences(sequences, max_length, pad_value):\n",
        "  result = list()\n",
        "  for sequence in sequences:\n",
        "    sequence = sequence[:max_length]\n",
        "    pad_langth = max_length - len(sequence)\n",
        "    padded_sequence = sequence + [pad_value] * pad_langth\n",
        "    result.append(padded_sequence)\n",
        "  return np.asarray(result)\n",
        "\n",
        "\n",
        "unk_id = token_to_id[\"<unk>\"]\n",
        "train_ids = [[token_to_id.get(token, unk_id)for token in review]for review in train_tokens]\n",
        "test_ids = [[token_to_id.get(token, unk_id)for token in review]for review in test_tokens]\n",
        "\n",
        "max_len = 32\n",
        "pad_id = token_to_id[\"<pad>\"]\n",
        "train_ids = pad_sequences(train_ids, max_len, pad_id)\n",
        "test_ids = pad_sequences(test_ids, max_len, pad_id)\n",
        "\n",
        "print(train_ids[0])\n",
        "print(test_ids[0])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxAA5XNwGDNh",
        "outputId": "8dc18ef8-9e4b-4e69-e09d-03013929ef85"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 223 1716   10 4036 2095  193  755    4    2 2330 1031  220   26   13\n",
            " 4839    1    1 8610    2    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0]\n",
            "[3307    5 1997  456    8    1 1013 3906    5 8802    1   13  223   51\n",
            "    3    1 4684    6    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# apply dataloader\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "train_ids = torch.tensor(train_ids)\n",
        "test_ids = torch.tensor(test_ids)\n",
        "\n",
        "train_labels = torch.tensor(train.label.values, dtype=torch.float32)\n",
        "test_labels = torch.tensor(test.label.values, dtype=torch.float32)\n",
        "\n",
        "train_dataset = TensorDataset(train_ids, train_labels)\n",
        "test_dataset = TensorDataset(test_ids, test_labels)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
      ],
      "metadata": {
        "id": "qKVKxzc1JR9j"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss function & optimization function\n",
        "from torch import optim\n",
        "\n",
        "n_vocab = len(token_to_id)\n",
        "hidden_dim = 64\n",
        "embedding_dim = 128\n",
        "n_layers = 2\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "classifier = SentenceClassifier(\n",
        "    n_vocab=n_vocab,\n",
        "    hidden_dim=hidden_dim, embedding_dim=embedding_dim, n_layers=n_layers\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss().to(device)\n",
        "optimizer = optim.Adam(classifier.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "OTIiB9f0J-TF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model & test\n",
        "def train(model, datasets, criterion, optimizer, device, interval):\n",
        "  model.train()\n",
        "  losses = list()\n",
        "\n",
        "  for step, (input_ids, labels) in enumerate(datasets):\n",
        "    input_ids = input_ids.to(device)\n",
        "    labels = labels.to(device).unsqueeze(1)\n",
        "\n",
        "    logits = model(input_ids)\n",
        "    loss = criterion(logits, labels)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if step % interval == 0:\n",
        "      print(f\"train loss : {step} : {np.mean(losses)}\")\n",
        "\n",
        "\n",
        "def test(model, datasets, criterion, device):\n",
        "  model.eval()\n",
        "  losses = list()\n",
        "  corrects = list()\n",
        "\n",
        "  for step, (input_ids, labels) in enumerate(datasets):\n",
        "    input_ids = input_ids.to(device)\n",
        "    labels = labels.to(device).unsqueeze(1)\n",
        "\n",
        "    logits = model(input_ids)\n",
        "    loss = criterion(logits, labels)\n",
        "    losses.append(loss.item())\n",
        "    yhat = torch.sigmoid(logits)>.5\n",
        "    corrects.extend(\n",
        "        torch.eq(yhat, labels).cpu().tolist()\n",
        "    )\n",
        "\n",
        "  print(f\"Val loss : {np.mean(losses)}, Val acc : {np.mean(corrects)}\")\n",
        "\n",
        "epochs = 5\n",
        "interval = 500\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  train(classifier, train_loader, criterion, optimizer, device, interval)\n",
        "  test(classifier, test_loader, criterion, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zZUV0rSLHJE",
        "outputId": "82a2a5e5-879c-4a4b-b2c1-349f65e4748b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss : 0 : 0.7063605785369873\n",
            "train loss : 500 : 0.6935788799902636\n",
            "train loss : 1000 : 0.6851173570522895\n",
            "train loss : 1500 : 0.6689832663829925\n",
            "train loss : 2000 : 0.6623453878123184\n",
            "train loss : 2500 : 0.6477210033016174\n",
            "Val loss : 0.5771354648251884, Val acc : 0.702\n",
            "train loss : 0 : 0.7620382905006409\n",
            "train loss : 500 : 0.5561112655612999\n",
            "train loss : 1000 : 0.5339300098506126\n",
            "train loss : 1500 : 0.5253059793261192\n",
            "train loss : 2000 : 0.5170289888151404\n",
            "train loss : 2500 : 0.5075790304653361\n",
            "Val loss : 0.4424886233604754, Val acc : 0.7966\n",
            "train loss : 0 : 0.3920058012008667\n",
            "train loss : 500 : 0.4161966366474024\n",
            "train loss : 1000 : 0.40629111306293386\n",
            "train loss : 1500 : 0.4010043893652229\n",
            "train loss : 2000 : 0.39706969481596405\n",
            "train loss : 2500 : 0.3929781673557279\n",
            "Val loss : 0.3970765081552652, Val acc : 0.8176\n",
            "train loss : 0 : 0.48149043321609497\n",
            "train loss : 500 : 0.3204819242471825\n",
            "train loss : 1000 : 0.32444011614649565\n",
            "train loss : 1500 : 0.3213781995362516\n",
            "train loss : 2000 : 0.32226512682014913\n",
            "train loss : 2500 : 0.32378864862391205\n",
            "Val loss : 0.39198299866324415, Val acc : 0.8236\n",
            "train loss : 0 : 0.12453757971525192\n",
            "train loss : 500 : 0.27743138075469737\n",
            "train loss : 1000 : 0.27785820211772316\n",
            "train loss : 1500 : 0.2805394211921749\n",
            "train loss : 2000 : 0.27951427629922165\n",
            "train loss : 2500 : 0.28191985052729884\n",
            "Val loss : 0.39557222124581903, Val acc : 0.8274\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 트랜스포머 모델 실습"
      ],
      "metadata": {
        "id": "3HIDlHmKJtFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchdata torchtext portalocker #torch data library\n",
        "!python -m spacy download de\n",
        "!python -m spacy download en"
      ],
      "metadata": {
        "id": "wR2MV41gOgKL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9eae0318-493c-45a4-c99e-27031d8fcb82"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchdata in /usr/local/lib/python3.10/dist-packages (0.7.1)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (0.18.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (2.10.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.31.0)\n",
            "Requirement already satisfied: torch>=2 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.3.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2->torchdata) (12.5.82)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2->torchdata) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2->torchdata) (1.3.0)\n",
            "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'de' are deprecated. Please use the\n",
            "full pipeline package name 'de_core_news_sm' instead.\u001b[0m\n",
            "Collecting de-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.7.0/de_core_news_sm-3.7.0-py3-none-any.whl (14.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from de-core-news-sm==3.7.0) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.12.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.8.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.25.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2024.6.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (13.7.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.18.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.14.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
            "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.8.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.25.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.6.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.7.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.18.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.14.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi30k 데이터세트가 다운로드 되지 않고, Timeout 오류가 발생하는 경우, 다음 셀을 실행한다.\n",
        "\n",
        "from torchtext.datasets import multi30k\n",
        "\n",
        "\n",
        "multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
        "multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
        "multi30k.URL[\"test\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/mmt16_task1_test.tar.gz\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXRMl6KiqI6d",
        "outputId": "e314ce2b-0d6e-43e2-fb4d-b93c5a1018aa"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchtext/datasets/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.10/dist-packages/torchtext/data/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#data set down load & preprocessing\n",
        "!pip install --upgrade torchdata torchtext\n",
        "from torchtext.datasets import Multi30k\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "!pip install --upgrade torch torchvision torchaudio\n",
        "from torchtext.datasets import Multi30k\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "def generate_tokens(text_iter, language):\n",
        "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
        "\n",
        "    for text in text_iter:\n",
        "        yield token_transform[language](text[language_index[language]])\n",
        "\n",
        "\n",
        "SRC_LANGUAGE = \"de\"\n",
        "TGT_LANGUAGE = \"en\"\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "special_symbols = [\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"]\n",
        "\n",
        "token_transform = {\n",
        "    SRC_LANGUAGE: get_tokenizer(\"spacy\", language=\"de_core_news_sm\"),\n",
        "    TGT_LANGUAGE: get_tokenizer(\"spacy\", language=\"en_core_web_sm\"),\n",
        "}\n",
        "print(\"Token Transform:\")\n",
        "print(token_transform)\n",
        "\n",
        "vocab_transform = {}\n",
        "for language in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    train_iter = Multi30k(split=\"train\", language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    vocab_transform[language] = build_vocab_from_iterator(\n",
        "        generate_tokens(train_iter, language),\n",
        "        min_freq=1,\n",
        "        specials=special_symbols,\n",
        "        special_first=True,\n",
        "    )\n",
        "\n",
        "for language in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    vocab_transform[language].set_default_index(UNK_IDX)\n",
        "\n",
        "print(\"Vocab Transform:\")\n",
        "print(vocab_transform)\n"
      ],
      "metadata": {
        "id": "ACA9zZDhJ3Vg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dc6648e-ed5f-45f6-e663-a4d6a8e55e2c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchdata in /usr/local/lib/python3.10/dist-packages (0.7.1)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (0.18.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.31.0)\n",
            "Requirement already satisfied: torch>=2 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.3.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2->torchdata) (12.5.82)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2->torchdata) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2->torchdata) (1.3.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.10/dist-packages/torchtext/utils.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.1)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Token Transform:\n",
            "{'de': functools.partial(<function _spacy_tokenize at 0x7b09070d05e0>, spacy=<spacy.lang.de.German object at 0x7b0906fb10c0>), 'en': functools.partial(<function _spacy_tokenize at 0x7b09070d05e0>, spacy=<spacy.lang.en.English object at 0x7b08f057bf40>)}\n",
            "Vocab Transform:\n",
            "{'de': Vocab(), 'en': Vocab()}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer model setup\n"
      ],
      "metadata": {
        "id": "aNGS1AQspNIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\n",
        "        )\n",
        "\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[: x.size(0)]\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
        "\n",
        "\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_encoder_layers,\n",
        "        num_decoder_layers,\n",
        "        emb_size,\n",
        "        max_len,\n",
        "        nhead,\n",
        "        src_vocab_size,\n",
        "        tgt_vocab_size,\n",
        "        dim_feedforward,\n",
        "        dropout=0.1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
        "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
        "        self.positional_encoding = PositionalEncoding(\n",
        "            d_model=emb_size, max_len=max_len, dropout=dropout\n",
        "        )\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=emb_size,\n",
        "            nhead=nhead,\n",
        "            num_encoder_layers=num_encoder_layers,\n",
        "            num_decoder_layers=num_decoder_layers,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        src,\n",
        "        trg,\n",
        "        src_mask,\n",
        "        tgt_mask,\n",
        "        src_padding_mask,\n",
        "        tgt_padding_mask,\n",
        "        memory_key_padding_mask,\n",
        "    ):\n",
        "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
        "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
        "        outs = self.transformer(\n",
        "            src=src_emb,\n",
        "            tgt=tgt_emb,\n",
        "            src_mask=src_mask,\n",
        "            tgt_mask=tgt_mask,\n",
        "            memory_mask=None,\n",
        "            src_key_padding_mask=src_padding_mask,\n",
        "            tgt_key_padding_mask=tgt_padding_mask,\n",
        "            memory_key_padding_mask=memory_key_padding_mask\n",
        "        )\n",
        "        return self.generator(outs)\n",
        "\n",
        "    def encode(self, src, src_mask):\n",
        "        return self.transformer.encoder(\n",
        "            self.positional_encoding(self.src_tok_emb(src)), src_mask\n",
        "        )\n",
        "\n",
        "    def decode(self, tgt, memory, tgt_mask):\n",
        "        return self.transformer.decoder(\n",
        "            self.positional_encoding(self.tgt_tok_emb(tgt)), memory, tgt_mask\n",
        "        )"
      ],
      "metadata": {
        "id": "L-QrWaL2r7Vl"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 파이토치에서 제공하는 트랜스포머 클래스를 활용하면 트랜스포머 구조의 모델을 쉽게 구현할 수 있다.\n",
        "transformer = torch.nn.Transformer(\n",
        "    d_model=512,\n",
        "    nhead=8,\n",
        "    num_encoder_layers=6,\n",
        "    num_decoder_layers=6,\n",
        "    dim_feedforward=2048,\n",
        "    dropout=0.1,\n",
        "    activation=torch.nn.functional.relu,\n",
        "    layer_norm_eps=1e-5,\n",
        "    batch_first=True,\n",
        ")"
      ],
      "metadata": {
        "id": "g-Ql4xe-sroW"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # 트랜스포머 순방향 메서드\n",
        "# output = transformer.forward(\n",
        "#     src,\n",
        "#     tgt,\n",
        "#     src_mask=None,\n",
        "#     tgt_mask=None,\n",
        "#     src_key_padding_mask=None,\n",
        "#     tgt_key_padding_mask=None,\n",
        "#     memory_key_padding_mask=None,\n",
        "# )"
      ],
      "metadata": {
        "id": "Xz2dve8BtNis"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import optim\n",
        "\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = Seq2SeqTransformer(\n",
        "    num_encoder_layers=3,\n",
        "    num_decoder_layers=3,\n",
        "    emb_size=512,\n",
        "    max_len=512,\n",
        "    nhead=8,\n",
        "    src_vocab_size=len(vocab_transform[SRC_LANGUAGE]),\n",
        "    tgt_vocab_size=len(vocab_transform[TGT_LANGUAGE]),\n",
        "    dim_feedforward=512,\n",
        ").to(DEVICE)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX).to(DEVICE)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "for main_name, main_module in model.named_children():\n",
        "    print(main_name)\n",
        "    for sub_name, sub_module in main_module.named_children():\n",
        "        print(\"└\", sub_name)\n",
        "        for ssub_name, ssub_module in sub_module.named_children():\n",
        "            print(\"│  └\", ssub_name)\n",
        "            for sssub_name, sssub_module in ssub_module.named_children():\n",
        "                print(\"│  │  └\", sssub_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7otD4eUEumqY",
        "outputId": "97efd790-32ee-4b67-d99e-ce7d3e56095f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "src_tok_emb\n",
            "└ embedding\n",
            "tgt_tok_emb\n",
            "└ embedding\n",
            "positional_encoding\n",
            "└ dropout\n",
            "transformer\n",
            "└ encoder\n",
            "│  └ layers\n",
            "│  │  └ 0\n",
            "│  │  └ 1\n",
            "│  │  └ 2\n",
            "│  └ norm\n",
            "└ decoder\n",
            "│  └ layers\n",
            "│  │  └ 0\n",
            "│  │  └ 1\n",
            "│  │  └ 2\n",
            "│  └ norm\n",
            "generator\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#배치 데이터 생성\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "\n",
        "def sequential_transforms(*transforms):\n",
        "    def func(txt_input):\n",
        "        for transform in transforms:\n",
        "            txt_input = transform(txt_input)\n",
        "        return txt_input\n",
        "    return func\n",
        "\n",
        "def input_transform(token_ids):\n",
        "    return torch.cat(\n",
        "        (torch.tensor([BOS_IDX]), torch.tensor(token_ids), torch.tensor([EOS_IDX]))\n",
        "    )\n",
        "\n",
        "def collator(batch):\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src_sample, tgt_sample in batch:\n",
        "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
        "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
        "\n",
        "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
        "    return src_batch, tgt_batch\n",
        "\n",
        "\n",
        "text_transform = {}\n",
        "for language in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    text_transform[language] = sequential_transforms(\n",
        "        token_transform[language], vocab_transform[language], input_transform\n",
        "    )\n",
        "\n",
        "data_iter = Multi30k(split=\"valid\", language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "dataloader = DataLoader(data_iter, batch_size=BATCH_SIZE, collate_fn=collator)\n",
        "source_tensor, target_tensor = next(iter(dataloader))\n",
        "\n",
        "print(\"(source, target):\")\n",
        "print(next(iter(data_iter)))\n",
        "\n",
        "print(\"source_batch:\", source_tensor.shape)\n",
        "print(source_tensor)\n",
        "\n",
        "print(\"target_batch:\", target_tensor.shape)\n",
        "print(target_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjxjOSoZtgpL",
        "outputId": "edfd4cfa-f20e-4c4b-ebd9-c274dd89a3a3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(source, target):\n",
            "('Eine Gruppe von Männern lädt Baumwolle auf einen Lastwagen', 'A group of men are loading cotton onto a truck')\n",
            "source_batch: torch.Size([35, 128])\n",
            "tensor([[   2,    2,    2,  ...,    2,    2,    2],\n",
            "        [  14,    5,    5,  ...,    5,   21,    5],\n",
            "        [  38,   12,   35,  ...,   12, 1750,   69],\n",
            "        ...,\n",
            "        [   1,    1,    1,  ...,    1,    1,    1],\n",
            "        [   1,    1,    1,  ...,    1,    1,    1],\n",
            "        [   1,    1,    1,  ...,    1,    1,    1]])\n",
            "target_batch: torch.Size([30, 128])\n",
            "tensor([[   2,    2,    2,  ...,    2,    2,    2],\n",
            "        [   6,    6,    6,  ...,  250,   19,    6],\n",
            "        [  39,   12,   35,  ...,   12, 3254,   61],\n",
            "        ...,\n",
            "        [   1,    1,    1,  ...,    1,    1,    1],\n",
            "        [   1,    1,    1,  ...,    1,    1,    1],\n",
            "        [   1,    1,    1,  ...,    1,    1,    1]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#어텐션 마스크 생성\n",
        "def generate_square_subsequent_mask(s):\n",
        "    mask = (torch.triu(torch.ones((s, s), device=DEVICE)) == 1).transpose(0, 1)\n",
        "    mask = (\n",
        "        mask.float()\n",
        "        .masked_fill(mask == 0, float(\"-inf\"))\n",
        "        .masked_fill(mask == 1, float(0.0))\n",
        "    )\n",
        "    return mask\n",
        "\n",
        "\n",
        "def create_mask(src, tgt):\n",
        "    src_seq_len = src.shape[0]\n",
        "    tgt_seq_len = tgt.shape[0]\n",
        "\n",
        "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
        "    src_mask = torch.zeros((src_seq_len, src_seq_len), device=DEVICE).type(torch.bool)\n",
        "\n",
        "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
        "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
        "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n",
        "\n",
        "\n",
        "target_input = target_tensor[:-1, :]\n",
        "target_out = target_tensor[1:, :]\n",
        "\n",
        "source_mask, target_mask, source_padding_mask, target_padding_mask = create_mask(\n",
        "    source_tensor, target_input\n",
        ")\n",
        "\n",
        "print(\"source_mask:\", source_mask.shape)\n",
        "print(source_mask)\n",
        "print(\"target_mask:\", target_mask.shape)\n",
        "print(target_mask)\n",
        "print(\"source_padding_mask:\", source_padding_mask.shape)\n",
        "print(source_padding_mask)\n",
        "print(\"target_padding_mask:\", target_padding_mask.shape)\n",
        "print(target_padding_mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1u_pXQPhuuOv",
        "outputId": "53b84d1a-29bf-4281-91c8-fe27791f5c84"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_mask: torch.Size([35, 35])\n",
            "tensor([[False, False, False,  ..., False, False, False],\n",
            "        [False, False, False,  ..., False, False, False],\n",
            "        [False, False, False,  ..., False, False, False],\n",
            "        ...,\n",
            "        [False, False, False,  ..., False, False, False],\n",
            "        [False, False, False,  ..., False, False, False],\n",
            "        [False, False, False,  ..., False, False, False]], device='cuda:0')\n",
            "target_mask: torch.Size([29, 29])\n",
            "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
            "         -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
            "         -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
            "         -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
            "         -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
            "         -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
            "         -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
            "         -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
            "         -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
            "         -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
            "         -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
            "         -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
            "         -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
            "         -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
            "         -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
            "         -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
            "         -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
            "         -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf,\n",
            "         -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf,\n",
            "         -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf,\n",
            "         -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
            "         -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf,\n",
            "         -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf,\n",
            "         -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0.]], device='cuda:0')\n",
            "source_padding_mask: torch.Size([128, 35])\n",
            "tensor([[False, False, False,  ...,  True,  True,  True],\n",
            "        [False, False, False,  ...,  True,  True,  True],\n",
            "        [False, False, False,  ...,  True,  True,  True],\n",
            "        ...,\n",
            "        [False, False, False,  ...,  True,  True,  True],\n",
            "        [False, False, False,  ...,  True,  True,  True],\n",
            "        [False, False, False,  ...,  True,  True,  True]])\n",
            "target_padding_mask: torch.Size([128, 29])\n",
            "tensor([[False, False, False,  ...,  True,  True,  True],\n",
            "        [False, False, False,  ...,  True,  True,  True],\n",
            "        [False, False, False,  ...,  True,  True,  True],\n",
            "        ...,\n",
            "        [False, False, False,  ...,  True,  True,  True],\n",
            "        [False, False, False,  ...,  True,  True,  True],\n",
            "        [False, False, False,  ...,  True,  True,  True]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#모델 학습 및 평가\n",
        "def run(model, optimizer, criterion, split):\n",
        "    model.train() if split == \"train\" else model.eval()\n",
        "    data_iter = Multi30k(split=split, language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    dataloader = DataLoader(data_iter, batch_size=BATCH_SIZE, collate_fn=collator)\n",
        "\n",
        "    losses = 0\n",
        "    for source_batch, target_batch in dataloader:\n",
        "        source_batch = source_batch.to(DEVICE)\n",
        "        target_batch = target_batch.to(DEVICE)\n",
        "\n",
        "        target_input = target_batch[:-1, :]\n",
        "        target_output = target_batch[1:, :]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(\n",
        "            source_batch, target_input\n",
        "        )\n",
        "\n",
        "        logits = model(\n",
        "            src=source_batch,\n",
        "            trg=target_input,\n",
        "            src_mask=src_mask,\n",
        "            tgt_mask=tgt_mask,\n",
        "            src_padding_mask=src_padding_mask,\n",
        "            tgt_padding_mask=tgt_padding_mask,\n",
        "            memory_key_padding_mask=src_padding_mask,\n",
        "        )\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(logits.reshape(-1, logits.shape[-1]), target_output.reshape(-1))\n",
        "        if split == \"train\":\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(list(dataloader))\n",
        "\n",
        "\n",
        "for epoch in range(5):\n",
        "    train_loss = run(model, optimizer, criterion, \"train\")\n",
        "    val_loss = run(model, optimizer, criterion, \"valid\")\n",
        "    print(f\"Epoch: {epoch+1}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27RrrRGyu1kg",
        "outputId": "08d41106-a64e-4b24-8164-e75fbcba4bbc"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5137: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Train loss: 4.683, Val loss: 3.979\n",
            "Epoch: 2, Train loss: 3.727, Val loss: 3.661\n",
            "Epoch: 3, Train loss: 3.431, Val loss: 3.557\n",
            "Epoch: 4, Train loss: 3.256, Val loss: 3.492\n",
            "Epoch: 5, Train loss: 3.139, Val loss: 3.487\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 트랜스포머 모델 번역 결과\n",
        "def greedy_decode(model, source_tensor, source_mask, max_len, start_symbol):\n",
        "    source_tensor = source_tensor.to(DEVICE)\n",
        "    source_mask = source_mask.to(DEVICE)\n",
        "\n",
        "    memory = model.encode(source_tensor, source_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
        "    for i in range(max_len - 1):\n",
        "        memory = memory.to(DEVICE)\n",
        "        target_mask = generate_square_subsequent_mask(ys.size(0))\n",
        "        target_mask = target_mask.type(torch.bool).to(DEVICE)\n",
        "\n",
        "        out = model.decode(ys, memory, target_mask)\n",
        "        out = out.transpose(0, 1)\n",
        "        prob = model.generator(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        next_word = next_word.item()\n",
        "\n",
        "        ys = torch.cat(\n",
        "            [ys, torch.ones(1, 1).type_as(source_tensor.data).fill_(next_word)], dim=0\n",
        "        )\n",
        "        if next_word == EOS_IDX:\n",
        "            break\n",
        "    return ys\n",
        "\n",
        "\n",
        "def translate(model, source_sentence):\n",
        "    model.eval()\n",
        "    source_tensor = text_transform[SRC_LANGUAGE](source_sentence).view(-1, 1)\n",
        "    num_tokens = source_tensor.shape[0]\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "    tgt_tokens = greedy_decode(\n",
        "        model, source_tensor, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX\n",
        "    ).flatten()\n",
        "    output = vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))[1:-1]\n",
        "    return \" \".join(output)\n",
        "\n",
        "\n",
        "output_oov = translate(model, \"Eine Gruppe von Menschen steht vor einem Iglu .\")\n",
        "output = translate(model, \"Eine Gruppe von Menschen steht vor einem Gebäude .\")\n",
        "print(output_oov)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oq9jYdNGu-5i",
        "outputId": "cce3ea51-cabc-4cef-d85b-b3efc7d4acfb"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A group of people are standing in a room .\n",
            "A group of people are standing in a room .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MoHn3NoEvFQN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}